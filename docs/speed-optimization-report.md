# Agent Pipeline Speed Optimization Report

## Baseline: ~60 seconds per issue (Issue #102, Run 22039735631)

| Phase | Time | % |
|-------|------|---|
| Ack job (separate runner) | ~6s | 10% |
| Git checkout | ~1s | 2% |
| Python setup | ~2s | 3% |
| **pip install (aider-chat!)** | **~25s** | **42%** |
| Git config | ~1s | 2% |
| LLM #1 Manager classify (gpt-4o) | ~5s | 8% |
| LLM #2 JuniorDev fix (gpt-4o) | ~4s | 7% |
| GitHub API (8x sequential) | ~4s | 7% |
| Test suite (52 tests) | ~2s | 3% |
| Git push + PR | ~3s | 5% |
| Source read (2x all files) | ~1s | 2% |
| Post-steps | ~6s | 10% |

## Research Sources

- **OpenAI Latency Guide** - 7 principles: faster model, fewer tokens, fewer requests, parallelize
- **uv (Astral)** - Rust pip replacement, 10-100x faster
- **GA4GC (UCL paper)** - Smaller models for classification in coding agents
- **GitHub Actions Cache** - Venv caching eliminates pip install on cache hit
- **SWE-bench** - Targeted file selection, minimal test scope
- **Adam Johnson** - Cached venvs pattern for GitHub Actions

---

## 30 Optimizations

| # | Optimization | Impact | Complexity | Effort | Known Pattern | Stability | Risk | Value-Add | Emoji |
|---|-------------|--------|------------|--------|---------------|-----------|------|-----------|-------|
| 1 | **Remove aider-chat from pip install** - not used, adds ~20s of deps | ‚ö°‚ö°‚ö° ~20s | üü¢ Trivial | 1 line | pip best practice | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 33% faster | üóëÔ∏è |
| 2 | **Remove MCP from requirements.txt** - agent doesn't use it | ‚ö°‚ö° ~3s | üü¢ Trivial | 1 line | Dep pruning | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 5% faster | üóëÔ∏è |
| 3 | **Remove python-dotenv** - not used in CI agent | ‚ö° ~1s | üü¢ Trivial | 1 line | Dep pruning | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 2% faster | üóëÔ∏è |
| 4 | **Pin exact versions in requirements.txt** - skip resolution | ‚ö° ~2s | üü¢ Easy | 5 lines | pip best practice | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 3% faster | üìå |
| 5 | **Use uv instead of pip** - 10-100x faster install | ‚ö°‚ö°‚ö° ~20s | üü° Medium | 3 lines | Astral uv | ‚ö†Ô∏è New tool | üü° 10% | üü¢ 33% faster | üöÄ |
| 6 | **Cache virtualenv in GH Actions** - skip install on hit | ‚ö°‚ö°‚ö° ~24s | üü° Medium | 10 lines | actions/cache | üõ°Ô∏è Proven | üü° 5% | üü¢ 40% faster | üíæ |
| 7 | **Merge ack + agent into single job** - skip 2nd runner | ‚ö°‚ö° ~5s | üü° Medium | 15 lines | GHA best practice | ‚ö†Ô∏è Moderate | üü° 10% | üü¢ 8% faster | üîó |
| 8 | **Use gpt-4o-mini for Manager classify** - faster model | ‚ö°‚ö° ~3s | üü¢ Trivial | 1 line | OpenAI latency guide | üõ°Ô∏è Proven | üü° 5% | üü¢ 5% faster | üß† |
| 9 | **Don't read sources twice** - pass sources from manager to junior | ‚ö° ~0.5s | üü¢ Easy | 3 lines | DRY principle | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 1% faster | ‚ôªÔ∏è |
| 10 | **Send only issue-referenced files** - not ALL src/glassbox/ | ‚ö°‚ö° ~2s | üü° Medium | 15 lines | SWE-bench pattern | ‚ö†Ô∏è Moderate | üü° 15% | üü¢ 3% faster | üéØ |
| 11 | **Set max_tokens on LLM calls** - cap runaway generation | ‚ö° ~1s | üü¢ Trivial | 1 line | OpenAI latency guide | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 2% faster | üîí |
| 12 | **Skip redundant syntax check** - tests already catch this | ‚ö° ~0.5s | üü¢ Trivial | 1 line | Test optimization | üõ°Ô∏è Rock solid | üü¢ 2% | üü¢ 1% faster | ‚è≠Ô∏è |
| 13 | **Reduce test verbosity** - remove -v flag | ‚ö° ~0.3s | üü¢ Trivial | 1 line | pytest best practice | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 0.5% faster | üîá |
| 14 | **Parallel LLM calls (speculative)** - start fix while classifying | ‚ö°‚ö° ~4s | üî¥ Hard | 30 lines | OpenAI parallelize | üí£ High | üî¥ 30% | üü° 7% faster | ‚ö° |
| 15 | **Use Predicted Outputs** - OpenAI feature for code edits | ‚ö°‚ö° ~2s | üü° Medium | 10 lines | OpenAI feature | ‚ö†Ô∏è Moderate | üü° 10% | üü° 3% faster | üîÆ |
| 16 | **Use httpx instead of gh CLI** - skip subprocess per API call | ‚ö° ~2s | üü† Significant | 50 lines | HTTP best practice | ‚ö†Ô∏è Moderate | üü° 15% | üü° 3% faster | üåê |
| 17 | **Sparse git checkout** - only src/ and tests/ | ‚ö° ~0.5s | üü¢ Easy | 3 lines | GHA optimization | üõ°Ô∏è Proven | üü¢ 2% | üü¢ 1% faster | üìÇ |
| 18 | **Pre-built Docker image** - eliminate pip install entirely | ‚ö°‚ö°‚ö° ~25s | üî¥ Hard | 30 lines | Docker CI pattern | ‚ö†Ô∏è Maintenance | üü° 10% | üü¢ 42% faster | üê≥ |
| 19 | **Shorter prompts** - trim boilerplate from classify/fix prompts | ‚ö° ~1s | üü° Medium | 10 lines | OpenAI: fewer input tokens | ‚ö†Ô∏è Moderate | üü° 10% | üü¢ 2% faster | ‚úÇÔ∏è |
| 20 | **Rule-based template selection** - regex not LLM for obvious issues | ‚ö°‚ö° ~5s | üü† Significant | 30 lines | SWE-bench pattern | üí£ High | üü° 20% | üü° 8% faster | üìè |
| 21 | **Combine Manager+JuniorDev into 1 LLM call** - classify+fix together | ‚ö°‚ö° ~4s | üü† Significant | 40 lines | OpenAI: fewer requests | üí£ High | üî¥ 25% | üü° 7% faster | üîÑ |
| 22 | **Cache LLM responses** - hash prompt, cache for identical issues | ‚ö°‚ö° ~8s | üü† Significant | 25 lines | LLM caching pattern | ‚ö†Ô∏è Moderate | üü° 10% | üü° 13% faster | üóÑÔ∏è |
| 23 | **Async GitHub API calls** - concurrent.futures for parallel calls | ‚ö° ~2s | üü° Medium | 20 lines | asyncio pattern | ‚ö†Ô∏è Moderate | üü° 10% | üü° 3% faster | üîÄ |
| 24 | **Run only affected tests** - not all 52 | ‚ö° ~1s | üü° Medium | 10 lines | pytest -k pattern | ‚ö†Ô∏è Moderate | üü° 15% | üü¢ 2% faster | üéØ |
| 25 | **Prefetch issue in ack job** - pass to agent, skip API call | ‚ö° ~1s | üü° Medium | 10 lines | GHA outputs | üõ°Ô∏è Proven | üü¢ 5% | üü¢ 2% faster | üì® |
| 26 | **Use gpt-4o-mini for BOTH calls** - fastest model | ‚ö°‚ö° ~5s | üü¢ Trivial | 1 line | OpenAI latency guide | ‚ö†Ô∏è Quality risk | üü° 20% | üü° 8% faster | üß† |
| 27 | **Stream LLM + process early** - start parsing as tokens arrive | ‚ö° ~1s | üü† Significant | 25 lines | OpenAI streaming | ‚ö†Ô∏è Moderate | üü° 10% | üü¢ 2% faster | üì° |
| 28 | **Remove pytest from agent deps** - use lightweight checker | ‚ö° ~1s | üü° Medium | 5 lines | Dep pruning | üí£ High | üî¥ 25% | üü¢ 2% faster | üßπ |
| 29 | **Use --no-compile for pip** - skip bytecode compilation | ‚ö° ~1s | üü¢ Trivial | 1 line | pip optimization | üõ°Ô∏è Rock solid | üü¢ 0% | üü¢ 2% faster | ‚è© |
| 30 | **Warm pip cache with lockfile** - pip freeze > lockfile | ‚ö° ~2s | üü° Medium | 5 lines | pip best practice | üõ°Ô∏è Proven | üü¢ 2% | üü¢ 3% faster | üîê |

---

## Shortlisted Top 10

Selected by: highest (impact x stability) / (complexity x risk)

| Rank | # | Optimization | Saved | Agent-fixable? |
|------|---|-------------|-------|---------------|
| 1 | 1 | Remove aider-chat from pip install | ~20s | Yes (1 line in YAML) |
| 2 | 2 | Remove MCP from requirements.txt | ~3s | Yes (1 line) |
| 3 | 8 | Use gpt-4o-mini for Manager classify | ~3s | Yes (1 line in settings.py) |
| 4 | 9 | Don't read sources twice in cli.py | ~0.5s | Yes (3 lines) |
| 5 | 11 | Set max_tokens on LLM calls | ~1s | Yes (1 line in base_agent.py) |
| 6 | 12 | Skip redundant syntax check | ~0.5s | Yes (1 line) |
| 7 | 6 | Cache virtualenv in GH Actions | ~24s | Manual (workflow change) |
| 8 | 4 | Pin exact versions in requirements.txt | ~2s | Yes (5 lines) |
| 9 | 29 | Use --no-compile for pip | ~1s | Yes (1 line in YAML) |
| 10 | 13 | Remove test verbosity -v flag | ~0.3s | Yes (1 line) |

**Projected total savings: ~55s (from ~60s to ~5-8s on cache hit, ~20-25s on cache miss)**

---

## PROOF: Before vs After

### Before (Issue #102, Run 22039735631)
- **Total workflow: ~60s**
- pip install: ~25s (aider-chat + mcp + loose versions)
- LLM calls: ~9s (2x gpt-4o, no max_tokens)
- Sources read twice
- No venv caching

### After (Issue #124, Run 22040259493)
- **Total workflow: ~32s (cache hit)**
- pip install: SKIPPED (venv cached)
- LLM calls: ~6s (gpt-4o-mini for classify, max_tokens=2048)
- Sources read once
- Venv cached with requirements.txt hash

### Progression
| Run | Issue | Time | What changed |
|-----|-------|------|--------------|
| 22039735631 | #102 (baseline) | **60s** | Before any optimization |
| 22039979570 | #104 | **54s** | - |
| 22040006649 | #106 | **24s** | aider-chat removed |
| 22040062934 | #108 | **42s** | venv cache (first run, cache miss) |
| 22040121261 | #110 | **29s** | venv cache hit |
| 22040142124 | #112 | **32s** | cache miss (requirements.txt changed) |
| 22040186021 | #114 | **32s** | cache hit |
| 22040200824 | #118 | **32s** | cache hit |
| 22040239621 | #122 | **34s** | cache miss (requirements.txt changed) |
| 22040259493 | #124 | **32s** | **FINAL: cache hit, all optimizations** |

### Delta
| Metric | Before | After | Saved |
|--------|--------|-------|-------|
| Total workflow time | 60s | 32s | **28s (47% faster)** |
| pip install | ~25s | ~0s (cached) | **25s** |
| LLM classify model | gpt-4o | gpt-4o-mini | **~2-3s** |
| Source file reads | 2x | 1x | **~0.5s** |
| Max tokens | unlimited | 2048 | **~1s** |

## Changes Made

### By Agent (PRs merged)
| PR | Issue | Change |
|----|-------|--------|
| #105 | #104 | Remove aider-chat from pip install |
| #107 | #106 | Remove MCP from requirements.txt |
| #111 | #110 | Remove -v verbose flag from test runner |
| #113 | #112 | Pin openai==1.82.0 |
| #117 | #116 | Pin pydantic==2.11.1 |
| #119 | #118 | Pin pyyaml==6.0.2 |
| #121 | #120 | Pin pytest==8.3.5 |
| #123 | #122 | Pin python-dotenv==1.1.0 |
| #125 | #124 | Rename workflow step v2 to v1 |

### Manual (pushed directly)
| Commit | Change |
|--------|--------|
| `0eedbec` | Venv caching with actions/cache@v4 + requirements.txt hash key |
| `2fc304a` | gpt-4o-mini for classify + reuse sources + max_tokens=2048 |
