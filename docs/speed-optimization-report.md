# Agent Pipeline Speed Optimization Report

## Baseline: ~60 seconds per issue (Issue #102, Run 22039735631)

| Phase | Time | % |
|-------|------|---|
| Ack job (separate runner) | ~6s | 10% |
| Git checkout | ~1s | 2% |
| Python setup | ~2s | 3% |
| **pip install (aider-chat!)** | **~25s** | **42%** |
| Git config | ~1s | 2% |
| LLM #1 Manager classify (gpt-4o) | ~5s | 8% |
| LLM #2 JuniorDev fix (gpt-4o) | ~4s | 7% |
| GitHub API (8x sequential) | ~4s | 7% |
| Test suite (52 tests) | ~2s | 3% |
| Git push + PR | ~3s | 5% |
| Source read (2x all files) | ~1s | 2% |
| Post-steps | ~6s | 10% |

## Research Sources

- **OpenAI Latency Guide** - 7 principles: faster model, fewer tokens, fewer requests, parallelize
- **uv (Astral)** - Rust pip replacement, 10-100x faster
- **GA4GC (UCL paper)** - Smaller models for classification in coding agents
- **GitHub Actions Cache** - Venv caching eliminates pip install on cache hit
- **SWE-bench** - Targeted file selection, minimal test scope
- **Adam Johnson** - Cached venvs pattern for GitHub Actions

---

## 30 Optimizations

| # | Optimization | Impact | Complexity | Effort | Known Pattern | Stability | Risk | Value-Add | Emoji |
|---|-------------|--------|------------|--------|---------------|-----------|------|-----------|-------|
| 1 | **Remove aider-chat from pip install** - not used, adds ~20s of deps | âš¡âš¡âš¡ ~20s | ğŸŸ¢ Trivial | 1 line | pip best practice | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 33% faster | ğŸ—‘ï¸ |
| 2 | **Remove MCP from requirements.txt** - agent doesn't use it | âš¡âš¡ ~3s | ğŸŸ¢ Trivial | 1 line | Dep pruning | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 5% faster | ğŸ—‘ï¸ |
| 3 | **Remove python-dotenv** - not used in CI agent | âš¡ ~1s | ğŸŸ¢ Trivial | 1 line | Dep pruning | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 2% faster | ğŸ—‘ï¸ |
| 4 | **Pin exact versions in requirements.txt** - skip resolution | âš¡ ~2s | ğŸŸ¢ Easy | 5 lines | pip best practice | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 3% faster | ğŸ“Œ |
| 5 | **Use uv instead of pip** - 10-100x faster install | âš¡âš¡âš¡ ~20s | ğŸŸ¡ Medium | 3 lines | Astral uv | âš ï¸ New tool | ğŸŸ¡ 10% | ğŸŸ¢ 33% faster | ğŸš€ |
| 6 | **Cache virtualenv in GH Actions** - skip install on hit | âš¡âš¡âš¡ ~24s | ğŸŸ¡ Medium | 10 lines | actions/cache | ğŸ›¡ï¸ Proven | ğŸŸ¡ 5% | ğŸŸ¢ 40% faster | ğŸ’¾ |
| 7 | **Merge ack + agent into single job** - skip 2nd runner | âš¡âš¡ ~5s | ğŸŸ¡ Medium | 15 lines | GHA best practice | âš ï¸ Moderate | ğŸŸ¡ 10% | ğŸŸ¢ 8% faster | ğŸ”— |
| 8 | **Use gpt-4o-mini for Manager classify** - faster model | âš¡âš¡ ~3s | ğŸŸ¢ Trivial | 1 line | OpenAI latency guide | ğŸ›¡ï¸ Proven | ğŸŸ¡ 5% | ğŸŸ¢ 5% faster | ğŸ§  |
| 9 | **Don't read sources twice** - pass sources from manager to junior | âš¡ ~0.5s | ğŸŸ¢ Easy | 3 lines | DRY principle | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 1% faster | â™»ï¸ |
| 10 | **Send only issue-referenced files** - not ALL src/glassbox/ | âš¡âš¡ ~2s | ğŸŸ¡ Medium | 15 lines | SWE-bench pattern | âš ï¸ Moderate | ğŸŸ¡ 15% | ğŸŸ¢ 3% faster | ğŸ¯ |
| 11 | **Set max_tokens on LLM calls** - cap runaway generation | âš¡ ~1s | ğŸŸ¢ Trivial | 1 line | OpenAI latency guide | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 2% faster | ğŸ”’ |
| 12 | **Skip redundant syntax check** - tests already catch this | âš¡ ~0.5s | ğŸŸ¢ Trivial | 1 line | Test optimization | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 2% | ğŸŸ¢ 1% faster | â­ï¸ |
| 13 | **Reduce test verbosity** - remove -v flag | âš¡ ~0.3s | ğŸŸ¢ Trivial | 1 line | pytest best practice | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 0.5% faster | ğŸ”‡ |
| 14 | **Parallel LLM calls (speculative)** - start fix while classifying | âš¡âš¡ ~4s | ğŸ”´ Hard | 30 lines | OpenAI parallelize | ğŸ’£ High | ğŸ”´ 30% | ğŸŸ¡ 7% faster | âš¡ |
| 15 | **Use Predicted Outputs** - OpenAI feature for code edits | âš¡âš¡ ~2s | ğŸŸ¡ Medium | 10 lines | OpenAI feature | âš ï¸ Moderate | ğŸŸ¡ 10% | ğŸŸ¡ 3% faster | ğŸ”® |
| 16 | **Use httpx instead of gh CLI** - skip subprocess per API call | âš¡ ~2s | ğŸŸ  Significant | 50 lines | HTTP best practice | âš ï¸ Moderate | ğŸŸ¡ 15% | ğŸŸ¡ 3% faster | ğŸŒ |
| 17 | **Sparse git checkout** - only src/ and tests/ | âš¡ ~0.5s | ğŸŸ¢ Easy | 3 lines | GHA optimization | ğŸ›¡ï¸ Proven | ğŸŸ¢ 2% | ğŸŸ¢ 1% faster | ğŸ“‚ |
| 18 | **Pre-built Docker image** - eliminate pip install entirely | âš¡âš¡âš¡ ~25s | ğŸ”´ Hard | 30 lines | Docker CI pattern | âš ï¸ Maintenance | ğŸŸ¡ 10% | ğŸŸ¢ 42% faster | ğŸ³ |
| 19 | **Shorter prompts** - trim boilerplate from classify/fix prompts | âš¡ ~1s | ğŸŸ¡ Medium | 10 lines | OpenAI: fewer input tokens | âš ï¸ Moderate | ğŸŸ¡ 10% | ğŸŸ¢ 2% faster | âœ‚ï¸ |
| 20 | **Rule-based template selection** - regex not LLM for obvious issues | âš¡âš¡ ~5s | ğŸŸ  Significant | 30 lines | SWE-bench pattern | ğŸ’£ High | ğŸŸ¡ 20% | ğŸŸ¡ 8% faster | ğŸ“ |
| 21 | **Combine Manager+JuniorDev into 1 LLM call** - classify+fix together | âš¡âš¡ ~4s | ğŸŸ  Significant | 40 lines | OpenAI: fewer requests | ğŸ’£ High | ğŸ”´ 25% | ğŸŸ¡ 7% faster | ğŸ”„ |
| 22 | **Cache LLM responses** - hash prompt, cache for identical issues | âš¡âš¡ ~8s | ğŸŸ  Significant | 25 lines | LLM caching pattern | âš ï¸ Moderate | ğŸŸ¡ 10% | ğŸŸ¡ 13% faster | ğŸ—„ï¸ |
| 23 | **Async GitHub API calls** - concurrent.futures for parallel calls | âš¡ ~2s | ğŸŸ¡ Medium | 20 lines | asyncio pattern | âš ï¸ Moderate | ğŸŸ¡ 10% | ğŸŸ¡ 3% faster | ğŸ”€ |
| 24 | **Run only affected tests** - not all 52 | âš¡ ~1s | ğŸŸ¡ Medium | 10 lines | pytest -k pattern | âš ï¸ Moderate | ğŸŸ¡ 15% | ğŸŸ¢ 2% faster | ğŸ¯ |
| 25 | **Prefetch issue in ack job** - pass to agent, skip API call | âš¡ ~1s | ğŸŸ¡ Medium | 10 lines | GHA outputs | ğŸ›¡ï¸ Proven | ğŸŸ¢ 5% | ğŸŸ¢ 2% faster | ğŸ“¨ |
| 26 | **Use gpt-4o-mini for BOTH calls** - fastest model | âš¡âš¡ ~5s | ğŸŸ¢ Trivial | 1 line | OpenAI latency guide | âš ï¸ Quality risk | ğŸŸ¡ 20% | ğŸŸ¡ 8% faster | ğŸ§  |
| 27 | **Stream LLM + process early** - start parsing as tokens arrive | âš¡ ~1s | ğŸŸ  Significant | 25 lines | OpenAI streaming | âš ï¸ Moderate | ğŸŸ¡ 10% | ğŸŸ¢ 2% faster | ğŸ“¡ |
| 28 | **Remove pytest from agent deps** - use lightweight checker | âš¡ ~1s | ğŸŸ¡ Medium | 5 lines | Dep pruning | ğŸ’£ High | ğŸ”´ 25% | ğŸŸ¢ 2% faster | ğŸ§¹ |
| 29 | **Use --no-compile for pip** - skip bytecode compilation | âš¡ ~1s | ğŸŸ¢ Trivial | 1 line | pip optimization | ğŸ›¡ï¸ Rock solid | ğŸŸ¢ 0% | ğŸŸ¢ 2% faster | â© |
| 30 | **Warm pip cache with lockfile** - pip freeze > lockfile | âš¡ ~2s | ğŸŸ¡ Medium | 5 lines | pip best practice | ğŸ›¡ï¸ Proven | ğŸŸ¢ 2% | ğŸŸ¢ 3% faster | ğŸ” |

---

## Shortlisted Top 10

Selected by: highest (impact x stability) / (complexity x risk)

| Rank | # | Optimization | Saved | Agent-fixable? |
|------|---|-------------|-------|---------------|
| 1 | 1 | Remove aider-chat from pip install | ~20s | Yes (1 line in YAML) |
| 2 | 2 | Remove MCP from requirements.txt | ~3s | Yes (1 line) |
| 3 | 8 | Use gpt-4o-mini for Manager classify | ~3s | Yes (1 line in settings.py) |
| 4 | 9 | Don't read sources twice in cli.py | ~0.5s | Yes (3 lines) |
| 5 | 11 | Set max_tokens on LLM calls | ~1s | Yes (1 line in base_agent.py) |
| 6 | 12 | Skip redundant syntax check | ~0.5s | Yes (1 line) |
| 7 | 6 | Cache virtualenv in GH Actions | ~24s | Manual (workflow change) |
| 8 | 4 | Pin exact versions in requirements.txt | ~2s | Yes (5 lines) |
| 9 | 29 | Use --no-compile for pip | ~1s | Yes (1 line in YAML) |
| 10 | 13 | Remove test verbosity -v flag | ~0.3s | Yes (1 line) |

**Projected total savings: ~55s (from ~60s to ~5-8s on cache hit, ~20-25s on cache miss)**
