# GlassBox AI ğŸ’

**Multi-agent MCP server with trust scoring and multi-round debate.**

[![PyPI](https://img.shields.io/pypi/v/glassbox-ai)](https://pypi.org/project/glassbox-ai/)
[![Tests](https://img.shields.io/badge/tests-20%2F20-brightgreen)]()
[![Python](https://img.shields.io/badge/python-3.10%2B-blue)]()
[![License: MIT](https://img.shields.io/badge/license-MIT-green)]()

<!-- mcp-name: io.github.glassbox-ai-labs/glassbox-ai -->

3 AI agents debate any topic, reference each other by name, agree/disagree, converge on a plan, and **trust scores update based on who persuades whom**.

```
â”â” ROUND 3 â”â”
ğŸ”µ @architect: HOLDING â€” scalability concerns remain valid.
ğŸŸ¢ @pragmatist: CHANGED â€” @critic's security points convinced me.
ğŸŸ¡ @critic: HOLDING â€” my edge cases stand.

â”â” TRUST UPDATES â”â”
ğŸ“Š @critic 0.85 â†’ 0.86 â†‘ (persuaded @pragmatist)
ğŸ“Š @architect â€” HELD position (no trust change)
```

---

## ğŸš€ Install

### Option 1: pip (recommended)
```bash
pip install glassbox-ai
```

### Option 2: From source
```bash
git clone https://github.com/glassbox-ai-labs/glassbox-ai
cd glassbox-ai
pip install -r requirements.txt
```

---

## âš¡ Quick Start â€” Windsurf / Cursor / Claude Desktop

Add to your MCP config (`~/.codeium/windsurf/mcp_config.json`):

### Production (auto-updates from PyPI):
```json
{
  "mcpServers": {
    "glassbox-ai": {
      "command": "glassbox-ai",
      "args": [],
      "env": { "OPENAI_API_KEY": "sk-..." }
    }
  }
}
```

### Dev (hot-reload, no restart needed):
Requires Python 3.12+ and `pip install mcp-hmr`:
```json
{
  "mcpServers": {
    "glassbox-ai": {
      "command": "mcp-hmr",
      "args": ["path/to/scripts/hmr_entry.py:mcp"],
      "env": { "OPENAI_API_KEY": "sk-..." }
    }
  }
}
```

Then ask your AI assistant anything â€” it will automatically use GlassBox tools.

---

## ï¿½ï¸ Tools

| Tool | Description |
|---|---|
| `debate(task)` | 3-round multi-agent debate with trust updates |
| `analyze(task, agents?)` | Parallel single-shot analysis with trust-weighted consensus |
| `trust_scores()` | View current trust scores for all agents |
| `update_trust(agent, was_correct)` | Manually adjust trust based on outcome |

---

## ğŸ¤– Agents

| Agent | Model | Role | Style |
|---|---|---|---|
| ğŸ”µ `@architect` | GPT-4o | Long-term, scalability | Direct, opinionated |
| ğŸŸ¢ `@pragmatist` | GPT-4o | Ship fast, iterate | Cuts scope, pushes back |
| ğŸŸ¡ `@critic` | GPT-4o-mini | Edge cases, security | Challenges assumptions |

---

## ğŸ”„ Debate Engine (V2)

3 rounds where agents talk **to each other**:

| Round | Instruction |
|---|---|
| **Round 1** | State your position |
| **Round 2** | React to others â€” agree/disagree with @names |
| **Round 3** | Final position â€” say CHANGED or HOLDING and why |

After Round 3:
- **LLM-as-judge** analyzes the transcript for genuine mind changes
- Trust scores update: persuader goes â†‘, with before/after deltas
- **Convergence summary** generated as an action plan

---

## ï¿½ Trust System

- **Persistence:** SQLite â€” scores survive across sessions
- **Initial score:** 0.85 for all agents
- **Update formula:** Exponential moving average: `new = old + 0.1 * (outcome - old)`
- **Floor/ceiling:** 0.30 â€“ 1.00
- **Triggers:** Debate persuasion (auto) or manual `update_trust` call

---

## ğŸ“‚ Project Structure

```
glassbox-ai/
â”œâ”€â”€ src/glassbox/
â”‚   â”œâ”€â”€ server.py          # MCP server â€” 4 tools
â”‚   â”œâ”€â”€ orchestrator.py    # Debate engine + parallel execution
â”‚   â””â”€â”€ trust_db.py        # SQLite trust persistence
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_glassbox.py   # 20 unit tests
â”‚   â””â”€â”€ test_integration.py# 5 integration tests (needs API key)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_local.sh       # Dev: Keychain + mcp-hmr hot reload
â”‚   â”œâ”€â”€ run_mcp.sh         # Docker runner
â”‚   â””â”€â”€ hmr_entry.py       # mcp-hmr wrapper for relative imports
â”œâ”€â”€ pyproject.toml         # PyPI config (v0.3.0)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ§ª Tests

```bash
pytest tests/ -v   # 20 passed, 5 skipped (integration needs API key)
```

---

## ğŸ—ºï¸ Roadmap

### âœ… Done
- [x] Multi-agent MCP server with 3 personas
- [x] Multi-round debate engine (3 rounds)
- [x] Trust scoring with SQLite persistence
- [x] LLM-as-judge mind-change detection
- [x] PyPI package (`pip install glassbox-ai`)
- [x] Docker image (GHCR)
- [x] CI pipeline (tests + Docker build)
- [x] Dev hot-reload via mcp-hmr
- [x] 20 unit tests passing

### ğŸ”² Next
- [ ] Pluggable debate protocols
- [ ] Confidence scores per agent response
- [ ] Bidirectional trust (agents rate each other)
- [ ] Early convergence detection (skip Round 3 if unanimous)
- [ ] Dynamic agent hiring (add specialist agents per topic)
- [ ] Web dashboard for trust evolution
- [ ] Multi-model support (Claude, Gemini)
- [ ] Claim verification layer

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Windsurf / Cursor / Claude Desktop  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ MCP (stdio)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  server.py â€” 4 tools                â”‚
â”‚  debate | analyze | trust | update   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  orchestrator.py                     â”‚
â”‚  3-round debate Â· LLM judge Â· V1    â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚           â”‚           â”‚
 ğŸ”µGPT-4o   ğŸŸ¢GPT-4o   ğŸŸ¡GPT-4o-mini
 @architect  @pragmatist @critic
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  trust_db.py â€” SQLite                â”‚
â”‚  EMA updates Â· floor 0.30 Â· cap 1.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— Related Work

- [Du et al. â€” "Improving Factuality and Reasoning via Multi-Agent Debate"](https://composable-models.github.io/llm_debate/)
- [Anthropic â€” Multi-Agent Research System](https://www.anthropic.com/engineering/multi-agent-research-system)
- [DebateLLM â€” Benchmarking Multi-Agent Debate](https://github.com/instadeepai/DebateLLM)
- [ICLR Blog â€” Multi-Agent Debate Frameworks](https://iclr-blogposts.github.io/2025/blog/mad/)

---

## ğŸ“œ License

MIT

---

## ğŸ“§ Contact

Built by [Sourabh Gupta](https://github.com/sourabharsh) Â· [GlassBox AI Labs](https://github.com/glassbox-ai-labs)

**ğŸ’ Glass Box over â¬› Black Box**
